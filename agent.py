"""
AutoML and SHAP Analysis Assistant Agent
ä½¿ç”¨agnoå’Œstreamlitæ„å»ºçš„AutoMLå’ŒSHAPåˆ†ææ™ºèƒ½ä½“
"""
import streamlit as st

from agno.agent import Agent
from agno.models.google import Gemini

from autogluon.tabular import TabularPredictor
from sklearn.model_selection import train_test_split
from autogluon.tabular import TabularPredictor
import shap

import pandas as pd
import numpy as np
import os
import tempfile
from typing import Dict, Any, Tuple
import pickle
import traceback

import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·


# è‡ªåŠ¨æœºå™¨å­¦ä¹ å‡½æ•°
def run_auto_ml(data: dict, 
                target: str, 
                problem_type: str = 'binary', 
                test_size: float = 0.2,
                random_state: int = 42,
                time_limit: int = 240) -> Dict[str, Any]:
    """
    ä½¿ç”¨AutoGluonè¿›è¡Œè‡ªåŠ¨æœºå™¨å­¦ä¹ å»ºæ¨¡
    
    å‚æ•°:
        data: è¾“å…¥æ•°æ®é›†ï¼Œpandas DataFrameæ ¼å¼
        target: ç›®æ ‡å˜é‡åç§°
        problem_type: é—®é¢˜ç±»å‹ï¼Œå¯é€‰å€¼ä¸º'binary'(äºŒåˆ†ç±»)ã€'multiclass'(å¤šåˆ†ç±»)æˆ–'regression'(å›å½’)
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
        
    è¿”å›:
        åŒ…å«æ¨¡å‹æ€§èƒ½ã€æœ€ä½³æ¨¡å‹å’Œç‰¹å¾é‡è¦æ€§çš„å­—å…¸
    """
    # æŒ‡å®šæ¨¡å‹ä¿å­˜è·¯å¾„
    save_path = f'agModels-{target}'
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    if os.path.exists(save_path):
        print(f"å‘ç°å·²æœ‰è®­ç»ƒæ¨¡å‹: {save_path}ï¼Œç›´æ¥åŠ è½½")
        try:
            predictor = TabularPredictor.load(save_path)
            print(f"æˆåŠŸåŠ è½½å·²æœ‰æ¨¡å‹ï¼Œç›®æ ‡å˜é‡: {predictor.label}")
            
            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            train_data, test_data = train_test_split(
                data, 
                test_size=test_size, 
                random_state=random_state, 
                stratify=data[target] if problem_type != 'regression' else None
            )
            
            leaderboard = predictor.leaderboard(test_data)
            performance = predictor.evaluate(test_data)
            best_model = predictor.model_best
            
            # è¿”å›ç»“æœ
            results = {
                "performance_metrics": performance,
                "best_model": best_model,
                "model_path": save_path,
                "leaderboard": leaderboard,
                "loaded_from_cache": True
            }
            
            return results
        except Exception as e:
            print(f"åŠ è½½å·²æœ‰æ¨¡å‹å¤±è´¥: {str(e)}ï¼Œå°†é‡æ–°è®­ç»ƒ")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_data, test_data = train_test_split(
        data, 
        test_size=test_size, 
        random_state=random_state, 
        stratify=data[target] if problem_type != 'regression' else None
    )
    
    print(f"\nè®­ç»ƒé›†å¤§å°: {train_data.shape[0]}è¡Œ")
    print(f"æµ‹è¯•é›†å¤§å°: {test_data.shape[0]}è¡Œ")
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = TabularPredictor(
        label=target,
        problem_type=problem_type,
        path=save_path
    ).fit(
        train_data,  # ä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒ
        save_space=True,  # åªä¿å­˜æœ€ä¼˜æ¨¡å‹
        presets='best_quality',  # ä½¿ç”¨æœ€ä½³è´¨é‡é¢„è®¾
        time_limit=time_limit,  # è®­ç»ƒæ—¶é—´é™åˆ¶ï¼ˆç§’ï¼‰
    )
    
    # è¯„ä¼°æ¨¡å‹æ€§èƒ½
    leaderboard = predictor.leaderboard(test_data)  # åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆæ’è¡Œæ¦œ
    print("\næ¨¡å‹æ’è¡Œæ¦œ (æµ‹è¯•é›†):")
    print(leaderboard)
    
    # è·å–æ€§èƒ½æŒ‡æ ‡
    performance = predictor.evaluate(test_data)  # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½
    print("\næ¨¡å‹æ€§èƒ½æŒ‡æ ‡ (æµ‹è¯•é›†):")
    print(performance)
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_model = predictor.model_best
    print("\næœ€ä½³æ¨¡å‹åç§°:", best_model)
    st.session_state.model_path = save_path
    print(f"é¢„æµ‹å™¨ä¿å­˜è·¯å¾„: {st.session_state.model_path}")

    # è¿”å›ç»“æœ
    results = {
        "performance_metrics": performance,
        "best_model": best_model if 'best_model' in locals() else None,
        "model_path": save_path,
        "leaderboard": leaderboard,
        "loaded_from_cache": False
    }
    
    return results

# SHAPåˆ†æå‡½æ•°
def run_shap_analysis(data: dict, 
                      target: str = None, 
                      model_path: str = None,
                      problem_type: str = 'binary',
                      max_display: int = 10,
                      sample_size: int = 20) -> Dict[str, Any]:
    """
    å¯¹è®­ç»ƒå¥½çš„AutoGluonæ¨¡å‹è¿›è¡ŒSHAPå¯è§£é‡Šæ€§åˆ†æ
    
    å‚æ•°:
        data: è¾“å…¥æ•°æ®é›†ï¼Œpandas DataFrameæ ¼å¼
        target: ç›®æ ‡å˜é‡åç§°
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        problem_type: é—®é¢˜ç±»å‹ï¼Œå¯é€‰å€¼ä¸º'binary'(äºŒåˆ†ç±»)ã€'multiclass'(å¤šåˆ†ç±»)æˆ–'regression'(å›å½’)
        max_display: æ˜¾ç¤ºçš„æœ€å¤§ç‰¹å¾æ•°é‡
        sample_size: ç”¨äºSHAPåˆ†æçš„æ ·æœ¬æ•°é‡ï¼Œè¾ƒå¤§çš„æ•°æ®é›†å¯ä»¥å‡å°‘æ ·æœ¬æ•°é‡ä»¥æé«˜æ€§èƒ½
        
    è¿”å›:
        åŒ…å«SHAPåˆ†æç»“æœçš„å­—å…¸
    """
    # data = st.session_state.data

    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    predictor = TabularPredictor.load(model_path)
    print(f"å·²åŠ è½½æ¨¡å‹ï¼Œç›®æ ‡å˜é‡: {predictor.label}")
    
    # å‡†å¤‡ç‰¹å¾æ•°æ®
    X = data.drop(columns=[target])
    
    # å¦‚æœæ•°æ®é›†è¾ƒå¤§ï¼ŒéšæœºæŠ½æ ·ä»¥æé«˜æ€§èƒ½
    if len(X) > sample_size:
        X_sample = X.sample(sample_size, random_state=42)
        print(f"æ•°æ®é›†è¾ƒå¤§ï¼ŒéšæœºæŠ½æ · {sample_size} æ¡è®°å½•è¿›è¡ŒSHAPåˆ†æ")
    else:
        X_sample = X
        print(f"ä½¿ç”¨å…¨éƒ¨ {len(X)} æ¡è®°å½•è¿›è¡ŒSHAPåˆ†æ")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model = predictor.model_best
    
    print(f"ä½¿ç”¨æ¨¡å‹ '{best_model}' è¿›è¡ŒSHAPåˆ†æ")

    # è®¡ç®—SHAP values
    try:
        # ä½¿ç”¨KernelExplainerä½œä¸ºå¤‡é€‰ï¼ˆé€‚ç”¨äºä»»ä½•æ¨¡å‹ï¼‰
        # åˆ›å»ºä¸€ä¸ªé¢„æµ‹å‡½æ•°
        def model_predict(X):
            return predictor.predict_proba(pd.DataFrame(X, columns=X_sample.columns))
        
        # ä½¿ç”¨èƒŒæ™¯æ•°æ®é›†
        background = shap.sample(X_sample, 50)  # ä½¿ç”¨50ä¸ªæ ·æœ¬ä½œä¸ºèƒŒæ™¯
        explainer = shap.KernelExplainer(model_predict, background)
        shap_values = explainer.shap_values(X_sample)
        shap_values_file_path = "./shap-values/shap_values.pkl"
        # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
        os.makedirs("./shap-values", exist_ok=True)
        
        # ä¿å­˜ shap_values åˆ°æ–‡ä»¶
        with open(shap_values_file_path, "wb") as f:
            pickle.dump(shap_values, f)
            print(f"å·²ä¿å­˜SHAPå€¼åˆ°æ–‡ä»¶: {shap_values_file_path}")
        
        # å¤„ç†å¤šåˆ†ç±»æƒ…å†µ
        if problem_type == 'multiclass':
            if isinstance(shap_values, list): # KernelExplainer for multiclass returns a list of arrays
                shap_values_list = shap_values
            elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 3: # Other explainers might return a 3D array
                # shap_values is (n_classes, n_samples, n_features) or (n_samples, n_features, n_classes)
                # We need a list of (n_samples, n_features) arrays, one for each class
                if shap_values.shape[0] == len(X_sample) : # (n_samples, n_features, n_classes)
                    shap_values_list = [shap_values[:, :, i] for i in range(shap_values.shape[2])]
                elif shap_values.shape[1] == len(X_sample): # (n_classes, n_samples, n_features)
                     shap_values_list = [shap_values[i, :, :] for i in range(shap_values.shape[0])]
                else:
                    raise ValueError(f"æ— æ³•å¤„ç†çš„SHAPå€¼å½¢çŠ¶: {shap_values.shape}")
            else:
                raise ValueError(f"å¤šåˆ†ç±»åœºæ™¯ä¸‹æ— æ³•å¤„ç†çš„SHAPå€¼ç±»å‹æˆ–å½¢çŠ¶: {type(shap_values)}")
            multi_class = True
        else: # Binary or regression
            shap_values_list = [shap_values] # Treat as a list with one element for consistency
            multi_class = False
            
    except Exception as e:
        raise Exception(f"SHAPåˆ†æå¤±è´¥: {str(e)}")
    
    # ç”ŸæˆSHAPå¯è§†åŒ–
    # æ¡å½¢æ‘˜è¦å›¾
    # For multiclass, shap.summary_plot can take a list of shap_values arrays
    # For binary/regression, shap_values_list will contain a single array
    plt.figure(figsize=(10, 8))
    summary_plot_path = "./figures/shap_summary.png" # å®šä¹‰è·¯å¾„å˜é‡
    if multi_class:
        shap.summary_plot(shap_values_list, X_sample, plot_type="bar", max_display=max_display, show=False, class_names=predictor.class_labels)
    else:
        shap.summary_plot(shap_values_list[0], X_sample, plot_type="bar", max_display=max_display, show=False)
    plt.title("ç‰¹å¾é‡è¦æ€§æ‘˜è¦å›¾")
    plt.tight_layout()
    plt.savefig(summary_plot_path) # ä½¿ç”¨è·¯å¾„å˜é‡ä¿å­˜
    plt.close()
    print(f"å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§æ‘˜è¦å›¾: {summary_plot_path}")

    # ç”ŸæˆSHAPä¾èµ–å›¾ï¼ˆé’ˆå¯¹æœ€é‡è¦çš„ç‰¹å¾ï¼‰
    all_top_features = set()
    dependence_plots_by_class = {}  # æ·»åŠ è¿™è¡Œæ¥å­˜å‚¨æ¯ä¸ªç±»åˆ«çš„ä¾èµ–å›¾ä¿¡æ¯

    for class_idx, shap_values_for_class in enumerate(shap_values_list):
        class_name = predictor.class_labels[class_idx] if multi_class and predictor.class_labels and class_idx < len(predictor.class_labels) else f"class_{class_idx}"
        print(f"ä¸ºç±»åˆ« '{class_name}' ç”Ÿæˆä¾èµ–å›¾...")
        
        # åˆå§‹åŒ–å½“å‰ç±»åˆ«çš„ä¾èµ–å›¾åˆ—è¡¨
        dependence_plots_by_class[class_name] = []

        # è·å–å½“å‰ç±»åˆ«çš„ç‰¹å¾é‡è¦æ€§æ’åº
        if shap_values_for_class.ndim == 1: # Workaround for single output regression from KernelExplainer
            feature_importance_for_class = np.abs(shap_values_for_class)
        else:
            feature_importance_for_class = np.abs(shap_values_for_class).mean(0)
        
        indices = np.argsort(feature_importance_for_class)
        
        # è·å–æœ€é‡è¦çš„ç‰¹å¾ (top 3 for each class)
        num_top_features_to_plot = min(3, len(X_sample.columns))
        top_indices_for_class = indices[-num_top_features_to_plot:]
        top_features_for_class = X_sample.columns[top_indices_for_class]
        all_top_features.update(top_features_for_class)

        for feature in top_features_for_class:
            plt.figure(figsize=(10, 7))
            shap.dependence_plot(feature, shap_values_for_class, X_sample, show=False, interaction_index="auto")
            title = f"ç‰¹å¾ä¾èµ–å›¾: {feature} (ç±»åˆ«: {class_name})"
            plt.title(title)
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            safe_feature_name = feature.replace('/', '_').replace('\\', '_')
            img_path = f"./figures/shap_dependence_{safe_feature_name}_class_{class_name}.png"
            plt.savefig(img_path)
            plt.close()
            print(f"  å·²ä¿å­˜ä¾èµ–å›¾: {img_path}")
            
            # å°†ä¾èµ–å›¾ä¿¡æ¯æ·»åŠ åˆ°å¯¹åº”ç±»åˆ«çš„åˆ—è¡¨ä¸­
            dependence_plots_by_class[class_name].append({
                "feature": feature,
                "path": img_path,
                "title": title
            })

    # calculate overall feature importance based on SHAP values
    # For overall feature importance, average absolute SHAP values across all classes if multiclass
    if multi_class:
        # Stack shap values for all classes and then average
        # Ensure all arrays in shap_values_list have the same shape before stacking
        # This might need adjustment if KernelExplainer output for multiclass has varying shapes (unlikely for tabular)
        abs_shap_values_stacked = np.abs(np.stack(shap_values_list, axis=0))
        overall_feature_importance = abs_shap_values_stacked.mean(axis=(0,1)) # Mean over classes and samples
    else:
        if shap_values_list[0].ndim == 1:
             overall_feature_importance = np.abs(shap_values_list[0])
        else:
            overall_feature_importance = np.abs(shap_values_list[0]).mean(0)
    
    # ä¿®æ”¹è¿”å›ç»“æœï¼Œæ·»åŠ dependence_plots_by_class
    results = {
        "feature_importance": dict(zip(X_sample.columns, overall_feature_importance)),
        "top_features_overall": list(all_top_features), 
        "multi_class": multi_class,
        "summary_plot_path": summary_plot_path,
        "dependence_plots_by_class": dependence_plots_by_class  # æ·»åŠ ä¾èµ–å›¾ä¿¡æ¯åˆ°è¿”å›ç»“æœ
    }
    
    return results

# åˆ›å»ºæ™ºèƒ½ä½“
def create_agents(api_key: str) -> Tuple[Agent, Agent]:
    # è‡ªåŠ¨æœºå™¨å­¦ä¹ æ™ºèƒ½ä½“
    auto_ml_agent = Agent(
        model=Gemini(
            id="gemini-2.0-flash", 
            api_key=api_key,
            system_prompt="""ä½ æ˜¯ä¸€ä½è‡ªåŠ¨æœºå™¨å­¦ä¹ ä¸“å®¶ï¼Œæ“…é•¿ä½¿ç”¨AutoGluonæ¡†æ¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
            1. è§£é‡ŠAutoGluonæ¨¡å‹è®­ç»ƒçš„ç»“æœ
            2. åˆ†ææ¨¡å‹æ€§èƒ½æŒ‡æ ‡
            3. è§£é‡Šæœ€ä½³æ¨¡å‹çš„é€‰æ‹©åŸå› 
            4. åˆ†æç‰¹å¾é‡è¦æ€§
            5. æä¾›æ¸…æ™°çš„ç»“æœè§£é‡Šå’Œå»ºè®®
            è¯·ç¡®ä¿è§£é‡Šä¸“ä¸šä¸”æ˜“äºç†è§£ã€‚"""
        ),
        markdown=True
    )
    
    # SHAPåˆ†ææ™ºèƒ½ä½“
    shap_agent = Agent(
        model=Gemini(
            id="gemini-2.0-flash",
            api_key=api_key,
            system_prompt="""ä½ æ˜¯ä¸€ä½æ¨¡å‹å¯è§£é‡Šæ€§ä¸“å®¶ï¼Œæ“…é•¿ä½¿ç”¨SHAPè¿›è¡Œæ¨¡å‹è§£é‡Šï¼Œå¹¶ä¸”ä½ èƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
            ä½ éœ€è¦åŸºäºSHAPå›¾æä¾›è¯¦ç»†ã€æ¸…æ™°ã€ä¸“ä¸šä¸”æ˜“äºç†è§£çš„å›¾æ–‡ç»“åˆçš„è§£é‡Šã€‚"""
        ),
        markdown=True
    )
    
    return auto_ml_agent, shap_agent

# æ„å»ºStreamlitåº”ç”¨
def build_app():
    """æ„å»ºAutoMLå’Œå¯è§£é‡Šæ€§åˆ†æçš„Streamlitåº”ç”¨"""
    st.title("ğŸ¤– AutoML and SHAP Analysis Assistant Agent")
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'api_key' not in st.session_state:
        st.session_state.api_key = ''
    if 'data' not in st.session_state:
        st.session_state.data = None
    if 'data_path' not in st.session_state:
        st.session_state.data_path = None
    if 'target' not in st.session_state:
        st.session_state.target = None
    if 'model_path' not in st.session_state:
        st.session_state.model_path = None
    if 'problem_type' not in st.session_state:
        st.session_state.problem_type = "binary"
    if 'history' not in st.session_state:
        st.session_state.history = []
    if 'summary_plot_base64' not in st.session_state:
        st.session_state.summary_plot_base64 = None
    if 'summary_plot_path' not in st.session_state:
        st.session_state.summary_plot_path = None
    

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.title("âš™ é…ç½®")
        st.session_state.api_key = st.text_input("Gemini APIå¯†é’¥", 
                                               value=st.session_state.api_key,
                                               type="password")

        # ä¸Šä¼ æ•°æ®æ–‡ä»¶
        uploaded_file = st.file_uploader("ä¸Šä¼ æ•°æ®æ–‡ä»¶", type=["csv", "xlsx"])
        if uploaded_file is not None:
            try:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜ä¸Šä¼ çš„æ•°æ®
                file_extension = os.path.splitext(uploaded_file.name)[1].lower()
                
                # åˆ›å»ºä¸€ä¸ªæŒä¹…çš„ä¸´æ—¶æ–‡ä»¶
                temp_dir = tempfile.gettempdir()
                temp_filename = f"uploaded_data_{uploaded_file.name}"
                temp_filepath = os.path.join(temp_dir, temp_filename)
                
                # ä¿å­˜ä¸Šä¼ çš„æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
                with open(temp_filepath, 'wb') as f:
                    f.write(uploaded_file.getvalue())
                
                # æ ¹æ®æ–‡ä»¶æ‰©å±•åè¯»å–æ•°æ®
                if file_extension == '.csv':
                    # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼è¯»å–CSVæ–‡ä»¶
                    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'iso-8859-1']
                    for encoding in encodings:
                        try:
                            data = pd.read_csv(temp_filepath, encoding=encoding)
                            st.success(f"æˆåŠŸä½¿ç”¨{encoding}ç¼–ç è¯»å–CSVæ–‡ä»¶")
                            break
                        except UnicodeDecodeError:
                            continue
                        except Exception as e:
                            st.error(f"ä½¿ç”¨{encoding}ç¼–ç è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                            continue
                    else:
                        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨äºŒè¿›åˆ¶æ–¹å¼è¯»å–
                        st.warning("æ— æ³•ç¡®å®šCSVæ–‡ä»¶çš„ç¼–ç ï¼Œå°è¯•ä½¿ç”¨äºŒè¿›åˆ¶æ–¹å¼è¯»å–")
                        data = pd.read_csv(temp_filepath, encoding='latin-1', on_bad_lines='skip')
                elif file_extension == '.xlsx':
                    data = pd.read_excel(temp_filepath)
                    st.session_state.data = pd.DataFrame(data)
                else:
                    st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
                    data = None
                
                if data is not None:
                    st.session_state.data = pd.DataFrame(data)
                    st.session_state.data_path = temp_filepath
                    st.success(f"æˆåŠŸåŠ è½½æ•°æ®: {data.shape[0]}è¡Œ x {data.shape[1]}åˆ—")
            except Exception as e:
                st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
    
    # ä¸»ç•Œé¢
    if st.session_state.data is not None:
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        st.subheader("ğŸ“Š æ•°æ®é¢„è§ˆ")
        st.dataframe(st.session_state.data.head())
        
        # é€‰æ‹©ç›®æ ‡å˜é‡
        target_options = st.session_state.data.columns.tolist()
        st.session_state.target = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡", options=target_options)
        
        # é€‰æ‹©é—®é¢˜ç±»å‹
        problem_type = st.radio(
            "é€‰æ‹©é—®é¢˜ç±»å‹",
            options=["binary", "multiclass", "regression"],
            index=0
        )
        st.session_state.problem_type = problem_type
        
        # è®¾ç½®è®­ç»ƒæ—¶é—´é™åˆ¶
        time_limit = st.slider("è®­ç»ƒæ—¶é—´é™åˆ¶(ç§’)", min_value=30, max_value=600, value=120, step=30)
        
        # åˆ›å»ºæ ‡ç­¾é¡µ
        tab1, tab2 = st.tabs(["AutoML", "SHAPåˆ†æ"])
        
        with tab1:
            st.subheader("AutoML")
            
            if st.button("ç‚¹å‡»è¿è¡Œ"):
                if st.session_state.api_key:
                    with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´..."):
                        try:
                            auto_ml_resluts = run_auto_ml(
                                data=st.session_state.data,
                                target=st.session_state.target,
                                problem_type=st.session_state.problem_type,
                                time_limit=time_limit  # æ·»åŠ æ—¶é—´é™åˆ¶å‚æ•°
                            )
                            st.success("Autogluonæ‰§è¡Œå®Œæˆï¼ç°åœ¨ç”±Geminiè¿›è¡Œè§£é‡Š...")
                            auto_ml_agent, _ = create_agents(st.session_state.api_key)
                            data = pd.DataFrame(st.session_state.data)
                            agent_input = f"""
                            è¯·ä½ æ ¹æ®è‡ªåŠ¨æœºå™¨å­¦ä¹ è¿è¡Œçš„ç»“æœï¼Œæä¾›è¯¦ç»†çš„ç»“æœè§£é‡Šå’Œå»ºè®®ã€‚
                            è¯·ä½¿ç”¨ä»¥ä¸‹ä¿¡æ¯ï¼š
                            - æ•°æ®: {data}
                            - ç›®æ ‡å˜é‡: {st.session_state.target}
                            - é—®é¢˜ç±»å‹: {st.session_state.problem_type}
                            - è‡ªåŠ¨æœºå™¨å­¦ä¹ ç»“æœ: {auto_ml_resluts}
                            """
                            response = auto_ml_agent.run(agent_input)
                            st.markdown(response.content)
                        except Exception as e:
                            st.error(f"è¿è¡Œè‡ªåŠ¨æœºå™¨å­¦ä¹ æ—¶å‡ºé”™: {str(e)}")
                else:
                    st.error("è¯·å…ˆè®¾ç½®Gemini APIå¯†é’¥")
        
        with tab2:
            st.subheader("SHAPåˆ†æ")
            model_path_input = st.text_input("è¯·è¾“å…¥è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„", value=st.session_state.model_path or "")
            if model_path_input:
                st.session_state.model_path = model_path_input

            if st.button("è¿è¡ŒSHAPåˆ†æ"):
                if not st.session_state.api_key:
                    st.error("è¯·è¾“å…¥Gemini APIå¯†é’¥æ‰èƒ½è¿›è¡ŒSHAPç»“æœè§£é‡Šã€‚")
                elif st.session_state.model_path and st.session_state.data is not None and st.session_state.target:
                    with st.spinner("æ­£åœ¨è¿›è¡ŒSHAPåˆ†æå¹¶è¯·æ±‚Geminiè§£é‡Š..."):
                        try:
                            # è¿è¡ŒSHAPåˆ†æ
                            shap_results = run_shap_analysis(
                                data=st.session_state.data,
                                target=st.session_state.target,
                                model_path=st.session_state.model_path,
                                problem_type=st.session_state.problem_type
                            )
                            st.success("SHAPåˆ†æå®Œæˆï¼ç°åœ¨ç”±Geminiè¿›è¡Œè§£é‡Š...")
                            
                            # åˆ›å»ºSHAPæ™ºèƒ½ä½“
                            _, shap_agent = create_agents(st.session_state.api_key)
                            
                            # å‡†å¤‡å›¾åƒè·¯å¾„åˆ—è¡¨
                            image_paths = []
                            
                            # æ·»åŠ æ‘˜è¦å›¾
                            if shap_results.get("summary_plot_path"):
                                image_paths.append(shap_results["summary_plot_path"])
                            
                            # æ·»åŠ æ‰€æœ‰ä¾èµ–å›¾
                            if shap_results.get("dependence_plots_by_class"):
                                for class_name, plots_data_list in shap_results["dependence_plots_by_class"].items():
                                    for plot_data in plots_data_list:
                                        if plot_data.get("path"):
                                            image_paths.append(plot_data["path"])
                           
                            # æ˜¾ç¤ºåˆ†æç»“æœ
                            st.markdown("### SHAPåˆ†æç»“æœè§£é‡Š")
                            st.markdown("#### SHAPç‰¹å¾æ‘˜è¦å›¾")

                            prompt = "åˆ†æSHAPå›¾ï¼Œç»™å‡ºä¸“ä¸šçš„ï¼Œè¯¦ç»†çš„ï¼Œå¹¶ä¸”æœ‰è§è§£çš„è§£é‡Šï¼Œå¾—å‡ºçš„åˆ†æè¦æœ‰ä»·å€¼ï¼Œè€Œä¸æ˜¯æ³›æ³›è€Œè°ˆã€‚"
                            # æ˜¾ç¤ºæ‘˜è¦å›¾åŠå…¶è§£é‡Š
                            if shap_results.get("summary_plot_path"):
                                st.image(shap_results["summary_plot_path"], caption="ç‰¹å¾é‡è¦æ€§æ‘˜è¦å›¾")
                                print("Summary plot path:", shap_results["summary_plot_path"])
                                response = shap_agent.run(prompt, images=[{"filepath": shap_results["summary_plot_path"]}] )
                                st.markdown(response.content)

                            # æ˜¾ç¤ºä¾èµ–å›¾
                            if shap_results.get("dependence_plots_by_class"):
                                for class_name, plots_data_list in shap_results["dependence_plots_by_class"].items():
                                    st.markdown(f"#### ç±»åˆ«: {class_name}ä¸‹çš„é‡è¦ç‰¹å¾ä¾èµ–å›¾")
                                    for plot_data in plots_data_list:
                                        if plot_data.get("path"):
                                            feature_name = os.path.basename(plot_data["path"]).split("_class_")[0].replace("shap_dependence_", "")
                                            st.image(plot_data["path"], caption=f"ç‰¹å¾ä¾èµ–å›¾: {feature_name}")
                                            response = shap_agent.run(prompt, images=[{"filepath": plot_data["path"]}] )
                                            st.markdown(response.content)
                        
                        except Exception as e:
                            st.error(f"SHAPåˆ†ææˆ–Geminiè§£é‡Šè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                            st.text(traceback.format_exc())
                else:
                    st.error("è¯·å…ˆè®¾ç½®Gemini APIå¯†é’¥")
    else:
        st.info("è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶")

if __name__ == "__main__":
    build_app()
